{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f2e5a2",
   "metadata": {},
   "source": [
    "## Auto Grade in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "90d3cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "1761e06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find differntiation of x^2 \n",
    "\n",
    "def dy_dx(x):\n",
    "    return 2*x\n",
    "\n",
    "dy_dx(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "1026a4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.466781571308061"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# complication inc y = x^2 , z = sin(y) now write a program \n",
    "# chain rule of diff \n",
    "\n",
    "def dz_dx(x):\n",
    "    return 2*x * math.cos(x**2)\n",
    "\n",
    "dz_dx(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b40552",
   "metadata": {},
   "source": [
    "## Use autograde for upper equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051de31a",
   "metadata": {},
   "source": [
    "eg 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "4470530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0,requires_grad=True) # require grade tell pytorch we use differentiation\n",
    "\n",
    "y = x**2\n",
    "print(x)\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "23991c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() # find derivative of y\n",
    "x.grad # use to show derivative (dy/dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644d4e4",
   "metadata": {},
   "source": [
    "eg 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "c218d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(0.4121, grad_fn=<SinBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-5.4668)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = x^2 and z = sin(y) find dz/dx \n",
    "\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y = x**2\n",
    "z = torch.sin(y)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z) # it give value of z not diff of z\n",
    "\n",
    "z.backward() # it give derivative\n",
    "x.grad # must grad to starting one (dz/dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e2664",
   "metadata": {},
   "source": [
    "### Make a type of NN \n",
    "\n",
    "### x -> w -> (Sigmoid(b)) -> y-Pred --> Loss\n",
    "\n",
    "find dL/dw  // dL/db \n",
    "\n",
    "### 1. Linear Transformation : \n",
    "\n",
    "z = w.x + b\n",
    "\n",
    "### 2. Activation(sigmoid Function):\n",
    "\n",
    "y-Pred = sigma(z) = 1 /1 + e^-z\n",
    "\n",
    "### 3. Loss Function (Binary Cross Entropy Loss)\n",
    "\n",
    "L = -[y target.ln(y-pred) + (1 - y target).ln(1-ypred)]\n",
    "\n",
    "---\n",
    "### To Find\n",
    "\n",
    "### x --> w --> (Sigmoid(b)) --> y-Pred --> Loss\n",
    "\n",
    "1. dL/dw = dL/dy-Pred * d y-Pred/dz * dz/dw \n",
    "\n",
    "2. dL/db = dL/dy-Pred * d y-Pred/dz * dz/db\n",
    "\n",
    "3. yp = y-Pred\n",
    "\n",
    "4. dL/dyp = yp - y/yp (1-yp)  ,   dyp/dz = yp(1-yp)  ,  dz/dw = x  ,  dz/db = 1\n",
    "\n",
    "5. dL\\dw = (yp - y) * x   ,   dL/db = (yp - y)*1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03067b",
   "metadata": {},
   "source": [
    "## without auto grade how much it difficult to calculate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "be9129c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on data a student got 6.7 cgpa and not got placement \n",
    "#analysy and predict\n",
    "\n",
    "x = torch.tensor(6.7) #input feature\n",
    "y = torch.tensor(0.0) # True label (library)\n",
    "\n",
    "w = torch.tensor(1.0) #weight ( random value in w and b )\n",
    "b = torch.tensor(0.0) #Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "04c1391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross Entrophy Loss for scalar\n",
    "def binary_cross_entropy_loss(prediction,target):\n",
    "    epsilon = 1e-8 #to prevent log\n",
    "    prediction = torch.clamp(prediction,epsilon,1-epsilon)\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "4fdb929b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "z = w * x + b # weight sum(linear part)\n",
    "y_pred = torch.sigmoid(z) # predicted probability  ( sigmoid on z)\n",
    "\n",
    "# compute binary cross entropy loss\n",
    "loss = binary_cross_entropy_loss(y_pred,y)\n",
    "loss  # ans show loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "2fe5c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives : ( backpropogation)\n",
    "# 1. dL/d(y_pred) : Loss with repect to prediction (y_pred)\n",
    "\n",
    "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
    "\n",
    "#2. dy_pred/dz : prediction (y_pred) with respect to z (sigmoid derivatives)\n",
    "dy_pred_dz = y_pred * (1 - y_pred)\n",
    "\n",
    "#3. dz/dw and dz/db: z with respect to w and b\n",
    "dz_dw = x\n",
    "dz_db = 1 # dz/db = 1 (bias contribution directly to z)\n",
    "\n",
    "dl_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dl_db = dloss_dy_pred * dy_pred_dz * dz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "eae3e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradiant of loss w.r.t weight (dw) : 6.691762447357178\n",
      "Manual gradiant of loss w.r.t bias (db) : 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "print(f\"Manual gradiant of loss w.r.t weight (dw) : {dl_dw}\")\n",
    "print(f\"Manual gradiant of loss w.r.t bias (db) : {dl_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cc3f6",
   "metadata": {},
   "source": [
    "## With use of Autograde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "d1b79dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(6.7) #( req_grad = True) not put bcz we are not calculate wrt to x or y\n",
    "y = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "1adc3155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., requires_grad=True)\n",
      "tensor(0., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0,requires_grad=True)\n",
    "b = torch.tensor(0.0,requires_grad=True)\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "d1f633a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7000, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w * x + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "0f6b8855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = torch.sigmoid(z)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "73550114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate loss \n",
    "loss = binary_cross_entropy_loss(y_pred,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "039f3568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918)\n",
      "tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "# Derivative ( this step will be easy to find derivative use backward)\n",
    "# it is single NN it is very use full in big NN\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52822121",
   "metadata": {},
   "source": [
    "## Vector input tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "f8c40cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "d15dc233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x**2).mean() # y = f(x1,x2,x3)\n",
    "y  #(1^2 + 2^2 + 3^2)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "dc8f2899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad\n",
    "\n",
    "# y = (x1^2 + x2^2 + x3^2)/3\n",
    "\n",
    "# dy/dx1 = 2 x1/3 = 2/3 = 0.66\n",
    "# dy/dx2 = 2 x2/3 = 4/3 = 1.33\n",
    "# dy/dx3 = 2 x3/3 = 2 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e679f",
   "metadata": {},
   "source": [
    "## Clearing gradian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "6b70957a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we call grad multiple time cause problem\n",
    "x = torch.tensor(2.0,requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "b445ebe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass ( moving forward x -> y)\n",
    "y = x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "af365e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass ( x <- y)\n",
    "y.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "1a346fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad  # if we run forward pass and backward pass 2nd time then ans change 4 -> 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "864ec875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero grad ( reset and fix the prob )\n",
    "x.grad.zero_()  #  work as inplace =True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8401156",
   "metadata": {},
   "source": [
    "# Stop gradian tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "bd178db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "746b333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "f45a6935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "0b2311dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we train NN in that case we off bakward tracking as req only forward tracking\n",
    "\n",
    "#opt1 = required_grad_(False)\n",
    "#opt2 = detach()\n",
    "#opt3 = torch.no_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61100fb",
   "metadata": {},
   "source": [
    "### opt-1 (required_grad_(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "39848b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "82465b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(requires_grad=True) is not showing after 2 bcz we off bakward tracking\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "2378b0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we cannot call backward on y as req grad is not there and stop \n",
    "y = x**2\n",
    "y\n",
    "#y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac2073",
   "metadata": {},
   "source": [
    "### opt2 = detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "4189f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "5b61bfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make new tensor with function of x\n",
    "# z is exact copy of x but gradian tracking is not ON\n",
    "\n",
    "z = x.detach()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "46dbcd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x ** 2\n",
    "y # y has gradian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "4c76e09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = z ** 2\n",
    "y1  # y1 dont have grad funcion cannot do y1.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb79cd",
   "metadata": {},
   "source": [
    "### opt3 = torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "30cfe138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0,requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "fbffec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = x ** 2\n",
    "    \n",
    "# if we remove with statement then tracking will be there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "f70121c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y # gradian is off so cannot do backward tracking\n",
    "\n",
    "#y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815fa69",
   "metadata": {},
   "source": [
    " It depend on y as if x is req grad True then it do derivative , we have make it False or make a copy of x  in upper 2 cases while in 3 chase  with use of with torch.no_grad() it customize it\n",
    " \n",
    " --------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
